# 爬虫

[toc]

## 前言

1. 使用爬虫需要包含基本的头文件：

   ```python
   # 和网络请求相关
   import requests
   # 用于抓取页面中指定内容
   from lxml import etree
   # 和验证码识别相关
   import ddddocr
   
   # 其他
   import json
   import os
   ```

2. 爬取数据的步骤：

   1. 自定义请求头部和目标地址

      ```python
      
      ```

      



## 案例一：爬取百度搜索结果

关键：该案例包含request请求以及自定义请求参数。

```python
import requests

def search():
    # 1. 自定义请求头
    headers = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Mobile Safari/537.36'
    }
    # 2. 自定义请求主机
    url = 'http://cn.bing.com/search?'
    # 3. 设定参数
    q = input('请输入关键字：')
    pages = int(input('请输入页数：'))
    params = {
        'q': q,
        'first': (pages - 1) * 10
    }
    # 4. 获取返回信息，并获取返回信息的文本信息
    resp = requests.get(url=url, params=params, headers=headers)
    pageText = resp.text
    # 5. 将数据写入磁盘
    fileName = q + '.html'
    with open(fileName, 'w', encoding='utf-8') as fp:
        fp.write(pageText)
    print(fileName, "保存成功！")
    
    
if __name__ == '__main__':
    search()
```



## 案例二：爬取页面内某个网络请求

一个界面可能包含多个ajax请求，有时候我们不需要抓取整个界面，只需要抓取界面的某个ajax请求。

返回request请求的信息是json格式，此时需要修改接收参数，参考下述案例。

```python
def getFilm():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/99.0.4844.51 Mobile Safari/537.36 '
    }
    url = 'https://movie.douban.com/j/search_subjects?'
    params = {
        'type': 'movie',
        'tag': '最新',
        'source': 'index',
        'page_limit': 50,
        'page_start': 0
    }
    resp = requests.get(url=url, params=params, headers=headers)
    # 获取json数据
    dicObj = resp.json()
    # 写入json数据
    with open('resp.txt', 'w', encoding='utf-8') as fp:
        json.dump(obj=dicObj, fp=fp)
```





## 案例三：爬取必应搜索的标题

获取必应搜索的关键字标题。此时我们需要抓取HTML文本的某一类数据。用到的解析工具是etree。

可以在浏览器中直接获取某个元素的xpath值，也可以在浏览器中搜索xpath值。这对于调试非常方便。

```python
def useXpath():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Mobile Safari/537.36'
    }
    url = 'http://cn.bing.com/search?'
    q = input('请输入关键字：')
    pages = int(input('请输入页数：'))
    params = {
        'q': q,
        'first': (pages - 1) * 10
    }
    resp = requests.get(url=url, params=params, headers=headers)
    pageText = resp.text
    fileName = q + '.html'
    with open(fileName, 'w', encoding='utf-8') as fp:
        fp.write(pageText)
    print(fileName, "保存成功！")
	# 通过HTML函数解析文本得到 文档树(lxml.etree._Element) 类型的元素
    tree = etree.HTML(pageText)
    # xpath解析后得到的也是 文档树 类型的元素
    titleList = tree.xpath('//*[@id="b_results"]/li//div[1]/a/h2')
    print('len=', len(titleList))
    for title in titleList:
        # 将 lxml.etree._Element 转换成 str
        titleText = title.xpath('./text()')
        print(titleText)
    
    
if __name__ == '__main__':
    useXpath()
```



## 案例四：模拟登陆教务处

登录流程：

可以通过Chrome浏览器先分解一下登陆过程中会遇到哪些步骤。这里直接给出结论。

1. 访问教务处网址，此时我们得到网站返回的`验证码图片链接`和`cookie`信息。
2. 再次访问`验证码图片链接`，以字节流的方式将图片加载到本地，再通过本地的`ddddocr`库识别验证码。
3. 将验证码、cookie和个人信息通过post请求，再次发送给网站，即可登录成功。

思考：为什么第一步我们不直接获取 验证码图片，而要先获取 验证码图片的链接呢？



细节思考：

1. 如何获取cookies以及携带cookie呢？

   答：cookies本质上是包含在headers中的一个key-value键值对，我们可以直接使用`resp.headers.get('Set-Cookie')`直接获取对应的值(`resp = requeset(url)`)。或者使用Python自带的API`resp.cookies.items()`获取。

   使用时可以同时采用两种策略，根据实际情况选择合适的方法。

2. 如何携带cookies发送到服务器呢？

   答：同理，向headers中插入一个关于cookie的str即可。也可以调用Python自带的API（不过我个人不太建议）。



```python
def loginJWC():
    # 登录教务处

    # 建议将headers中的内容填写完整
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Host': '202.119.81.113:8080',
        'Origin': 'http://202.119.81.113:8080',
        'Referer': 'http://202.119.81.113:8080/',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/99.0.4844.51 Mobile Safari/537.36 ',
    }
    # 第一次请求地址
    url = 'http://202.119.81.113:8080'

    # 获取页面返回信息
    resp = requests.get(url=url, headers=headers)

    # 获取cookie(任选一种方法即可)
    # 方法一: 从响应标头中获取cookie
    # print('cookie1=', resp.headers.get('Set-Cookie'))
    # 方法二: 直接调用cookies参数
    ck = resp.cookies.items()[0]
    print('ck=', ck)
    # 将获取到的Cookie存入headers(也可以将Cookie传入request对象的post函数中，不过我不推荐)
    headers['Cookie'] = ck[0] + '=' + ck[1]

    # 获取页面文本
    pageText = resp.text
    # 将文本解析到文档树
    tree = etree.HTML(pageText)

    # 获取验证码地址
    imgPath = tree.xpath('//img[@id="SafeCodeImg"]/@src')[0]
    imgPath = url + str(imgPath)
    # 以 字节流 的方式获取图片信息
    img = requests.post(url=imgPath, headers=headers).content
    # 调用验证码识别库
    ocr = ddddocr.DdddOcr()
    code = ocr.classification(img=img)
    print('code=', code)
    with open('coding.jpg', 'w+b') as fp:
        fp.write(img)

    # 更新url为登录信息发送到的url
    url = 'http://202.119.81.113:8080/Logon.do'
    # 新增请求参数
    params = {
        'method': 'logon'
    }
    # 携带个人信息
    data = {
        'USERNAME': '920106840520',
        'PASSWORD': '123456789qwe',
        'RANDOMCODE': code
    }
    resp = requests.post(url=url, params=params, headers=headers, data=data)
    pageText = resp.text

    with open('jwc.html', 'w', encoding='utf-8') as fp:
        fp.write(pageText)

    print('headers=', resp.headers)

    if (len(resp.cookies) == 0):
        print('验证失败~')
        return
    print('验证成功！')

    newCk = resp.cookies.items()[0]
    print('newCk=', newCk)
    # 更新headers中的cookies
    headers['Cookie'] = newCk[0] + '=' + newCk[1]

    # 获取课表

    # 更新链接
    url = 'http://202.119.81.112:9080/njlgdx/xskb/xskb_list.do'
    # 更新请求参数
    params.clear()
    params['Ves632DSdyV'] = 'NEW_XSD_PYGL'

if __name__ == '__main__':
    loginJWC()
```



## 爬取教务处课程

```python
import time
from xml.etree import ElementTree
import requests
import ddddocr
from lxml import etree

cookies = {}


def login_jwc():
    # 使用while循环是为了防止验证码识别错误
    while True:
        global cookies
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'max-age=0',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
        }

        # 第一次获取cookie
        response = requests.get('http://202.119.81.113:8080/', headers=headers, verify=False)

        # 将第一次获取的cookie携带上，并发送登录请求
        cookies = {
            response.cookies.items()[0][0]: response.cookies.items()[0][1]
        }

        # 获取验证码
        headers = {
            'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Referer': 'http://202.119.81.113:8080/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
        }

        # 此时response是image格式
        response = requests.get('http://202.119.81.113:8080/verifycode.servlet', cookies=cookies, headers=headers, verify=False)

        # 保存验证码(生产环境中用不上)
        with open('a.png', 'wb') as fp:
            fp.write(response.content)

        orc = ddddocr.DdddOcr()
        code = orc.classification(img=response.content)
        print('code=' + code)

        # 登录 步骤1
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'max-age=0',
            'Origin': 'http://202.119.81.113:8080',
            'Proxy-Connection': 'keep-alive',
            'Referer': 'http://202.119.81.113:8080/',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
        }

        params = {
            'method': 'logon',
        }

        # USERNAME和PASSWORD根据实际情况获得
        data = {
            'USERNAME': '920106840520',
            'PASSWORD': '123456789qwe',
            'useDogCode': '',
            'RANDOMCODE': code,
        }

        response = requests.post(
            'http://202.119.81.113:8080/Logon.do',
            params=params,
            cookies=cookies,
            headers=headers,
            data=data,
            verify=False,
            allow_redirects=False  # 这一步很关键
        )

        if 'Location' in response.headers:
            break

        # 获取错误信息
        tree = etree.HTML(response.text)
        msg = tree.xpath('/html/body/form/div/div/div[2]/div[1]/font/text()')[0]
        if msg == '该帐号不存在或密码错误,请联系管理员!':
            print(msg)
            return

    # 登录 步骤2
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'max-age=0',
        'Proxy-Connection': 'keep-alive',
        'Referer': 'http://202.119.81.113:8080/',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
    }

    response = requests.get(
        response.headers['Location'],
        cookies=cookies,
        headers=headers,
        verify=False,
        allow_redirects=False  # 这一步很关键
    )

    # print(response.headers)
    # cookie_str 类似 JSESSIONID=EF37E2DB570ACCC8A0460F35911AAB78; Path=/njlgdx
    cookie_str = response.headers['Set-Cookie']
    sub_str = cookie_str.split(';')[0]

    # 登录 步骤3
    cookies = {
        sub_str.split('=')[0]: sub_str.split('=')[1]
    }

    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'max-age=0',
        'Proxy-Connection': 'keep-alive',
        'Referer': 'http://202.119.81.113:8080/',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
    }

    response = requests.get(response.headers['Location'], cookies=cookies, headers=headers, verify=False)

    #========================至此登录成功=========================
    # 携带上获取的cookie，即可在教务处中进行各种操作啦！
    text = response.text
    with open('jwc.html', 'w', encoding='utf-8') as fp:
        fp.write(text)


def get_course_table():
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'max-age=0',
        'Origin': 'http://202.119.81.112:9080',
        'Proxy-Connection': 'keep-alive',
        'Referer': 'http://202.119.81.112:9080/njlgdx/xskb/xskb_list.do?Ves632DSdyV=NEW_XSD_PYGL',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
    }

    params = {
        'Ves632DSdyV': 'NEW_XSD_PYGL',
    }

    data = [
        ('cj0701id', ''),
        ('zc', ''),
        ('demo', ''),
        ('xnxq01id', '2022-2023-2'),
        ('pageIndex', '1'),
    ]

    response = requests.post(
        'http://202.119.81.112:9080/njlgdx/xskb/xskb_list.do',
        params=params,
        cookies=cookies,
        headers=headers,
        data=data,
        verify=False,
    )

    text = response.text

    with open('class_table.html', 'w', encoding='utf-8') as fp:
        fp.write(text)

    tree = etree.HTML(text)

    lis = tree.xpath('//div[@class="kbcontent"]//text()')
    print(lis)


if __name__ == '__main__':
    start = time.time()
    login_jwc()
    get_course_table()
    end = time.time()
    print('运行时间：' + str(end - start) + '秒')

```





## 案例五：使用selenium模拟登陆教务处

```python
import time

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from PIL import Image
import ddddocr

opts = Options()
# 不能将加载策略写成normal，否则在无头模式下会加载失败（我也不知道为什么）
opts.page_load_strategy = 'normal'
# opts.add_argument('--headless')
service = Service('C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe')
wd = webdriver.Chrome(service=service, options=opts)


def visitJWC():
    wd.get('http://202.119.81.113:8080/')

    while True:
        # 通过xpath获取元素的操作要放在while循环里
        username = wd.find_element(By.XPATH, '/html/body/form/div/div/div[2]/div[1]/table/tbody/tr[2]/td[2]/input')
        password = wd.find_element(By.XPATH, '/html/body/form/div/div/div[2]/div[1]/table/tbody/tr[3]/td[2]/input[1]')
        rand_code = wd.find_element(By.XPATH, '/html/body/form/div/div/div[2]/div[1]/table/tbody/tr[4]/td[2]/input')
        username.clear()
        username.send_keys('9201068405208')
        password.clear()
        password.send_keys('123456789qwe')
        pic = wd.find_element(By.XPATH, '/html/body/form/div/div/div[2]/div[1]/table/tbody/tr[4]/td[2]/img')

        # 获取某个元素的截图
        is_pic = pic.screenshot('./code.jpg')
        if is_pic:
            print('截屏成功！')
        else:
            print('截屏失败~')
            return
        img = Image.open('./code.jpg')
        ocr = ddddocr.DdddOcr()
        code = ocr.classification(img=img)

        # 清空验证码，并填写新的验证码
        rand_code.clear()
        rand_code.send_keys(code)

        login_btn = wd.find_element(By.XPATH, '/html/body/form/div/div/div[2]/div[1]/table/tbody/tr[5]/td/input[1]')
        login_btn.click()

        try:
            wrong_tag = wd.find_element(By.XPATH, '/html/body/form/div/div/div[2]/div[1]/font')
        except:
            print('登录成功！')
            break
        else:
            print(wrong_tag.text)
            if wrong_tag.text == '该帐号不存在或密码错误,请联系管理员!':
                break


if __name__ == '__main__':
    start = time.time()
    visitJWC()
    end = time.time()
    print('运行时间：' + str(end - start) + '秒')
```



## 使用request登录教务处

```python
import time

import requests
import ddddocr

start = time.time()

headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Cache-Control': 'max-age=0',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
}

# 第一次获取cookie
response = requests.get('http://202.119.81.113:8080/', headers=headers, verify=False)

# 将第一次获取的cookie携带上，并发送登录请求
cookies = {
    response.cookies.items()[0][0]: response.cookies.items()[0][1]
}

# 获取验证码
headers = {
    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'Referer': 'http://202.119.81.113:8080/',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
}

# 此时response是image格式
response = requests.get('http://202.119.81.113:8080/verifycode.servlet', cookies=cookies, headers=headers, verify=False)

# 保存验证码
with open('a.png', 'wb') as fp:
    fp.write(response.content)

orc = ddddocr.DdddOcr()
code = orc.classification(img=response.content)
print('code=' + code)

# 登录 步骤1
headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Cache-Control': 'max-age=0',
    'Origin': 'http://202.119.81.113:8080',
    'Proxy-Connection': 'keep-alive',
    'Referer': 'http://202.119.81.113:8080/',
    'Upgrade-Insecure-Requests': '1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
}

params = {
    'method': 'logon',
}

data = {
    'USERNAME': '920106840520',
    'PASSWORD': '123456789qwe',
    'useDogCode': '',
    'RANDOMCODE': code,
}

response = requests.post(
    'http://202.119.81.113:8080/Logon.do',
    params=params,
    cookies=cookies,
    headers=headers,
    data=data,
    verify=False,
    allow_redirects=False  # 这一步很关键
)

# 登录 步骤2
headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Cache-Control': 'max-age=0',
    # 'Cookie': 'JSESSIONID=3E5339BDC4CAB28B50E826A5477C307F',
    'Proxy-Connection': 'keep-alive',
    'Referer': 'http://202.119.81.113:8080/',
    'Upgrade-Insecure-Requests': '1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
}

response = requests.get(
    response.headers['Location'],
    cookies=cookies,
    headers=headers,
    verify=False,
    allow_redirects=False  # 这一步很关键
)

# print(response.headers)
# cookie_str 类似 JSESSIONID=EF37E2DB570ACCC8A0460F35911AAB78; Path=/njlgdx
cookie_str = response.headers['Set-Cookie']
sub_str = cookie_str.split(';')[0]

# 登录 步骤3
cookies = {
    sub_str.split('=')[0]: sub_str.split('=')[1]
}

headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Cache-Control': 'max-age=0',
    'Proxy-Connection': 'keep-alive',
    'Referer': 'http://202.119.81.113:8080/',
    'Upgrade-Insecure-Requests': '1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
}

response = requests.get(response.headers['Location'], cookies=cookies, headers=headers, verify=False)

#========================至此登录成功=========================

# print(response.text)

end = time.time()

print('运行时间：' + str(end - start) + '秒。')

text = response.text
with open('a.html', 'w', encoding='utf-8') as fp:
    fp.write(text)
```





## 爬取教务处课表信息

```python
import copy
import time
from xml.etree import ElementTree
import requests
import ddddocr
from lxml import etree

cookies = {}


def login_jwc():
    # 使用while循环是为了防止验证码识别错误
    while True:
        global cookies
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'max-age=0',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
        }

        # 第一次获取cookie
        response = requests.get('http://202.119.81.113:8080/', headers=headers, verify=False)

        # 将第一次获取的cookie携带上，并发送登录请求
        cookies = {
            response.cookies.items()[0][0]: response.cookies.items()[0][1]
        }

        # 获取验证码
        headers = {
            'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Referer': 'http://202.119.81.113:8080/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
        }

        # 此时response是image格式
        response = requests.get('http://202.119.81.113:8080/verifycode.servlet', cookies=cookies, headers=headers, verify=False)

        # 保存验证码(生产环境中用不上)
        with open('a.png', 'wb') as fp:
            fp.write(response.content)

        orc = ddddocr.DdddOcr()
        code = orc.classification(img=response.content)
        print('code=' + code)

        # 登录 步骤1
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'max-age=0',
            'Origin': 'http://202.119.81.113:8080',
            'Proxy-Connection': 'keep-alive',
            'Referer': 'http://202.119.81.113:8080/',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
        }

        params = {
            'method': 'logon',
        }

        # USERNAME和PASSWORD根据实际情况获得
        data = {
            'USERNAME': '920106840520',
            'PASSWORD': '123456789qwe',
            'useDogCode': '',
            'RANDOMCODE': code,
        }

        response = requests.post(
            'http://202.119.81.113:8080/Logon.do',
            params=params,
            cookies=cookies,
            headers=headers,
            data=data,
            verify=False,
            allow_redirects=False  # 这一步很关键
        )

        if 'Location' in response.headers:
            break

        # 获取错误信息
        tree = etree.HTML(response.text)
        msg = tree.xpath('/html/body/form/div/div/div[2]/div[1]/font/text()')[0]
        if msg == '该帐号不存在或密码错误,请联系管理员!':
            print(msg)
            return

    # 登录 步骤2
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'max-age=0',
        'Proxy-Connection': 'keep-alive',
        'Referer': 'http://202.119.81.113:8080/',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
    }

    response = requests.get(
        response.headers['Location'],
        cookies=cookies,
        headers=headers,
        verify=False,
        allow_redirects=False  # 这一步很关键
    )

    # print(response.headers)
    # cookie_str 类似 JSESSIONID=EF37E2DB570ACCC8A0460F35911AAB78; Path=/njlgdx
    cookie_str = response.headers['Set-Cookie']
    sub_str = cookie_str.split(';')[0]

    # 登录 步骤3
    cookies = {
        sub_str.split('=')[0]: sub_str.split('=')[1]
    }

    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'max-age=0',
        'Proxy-Connection': 'keep-alive',
        'Referer': 'http://202.119.81.113:8080/',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
    }

    response = requests.get(response.headers['Location'], cookies=cookies, headers=headers, verify=False)

    #========================至此登录成功=========================
    # 携带上获取的cookie，即可在教务处中进行各种操作啦！
    text = response.text
    with open('jwc.html', 'w', encoding='utf-8') as fp:
        fp.write(text)


# 只解析 星期一(14-14小节 这样的字符串
def parse_single(s: str) -> list[int]:
    day = s.split('(')[0]
    if day == '星期一':
        day_res = 1
    elif day == '星期二':
        day_res = 2
    elif day == '星期三':
        day_res = 3
    elif day == '星期四':
        day_res = 4
    elif day == '星期五':
        day_res = 5
    elif day == '星期六':
        day_res = 6
    else:
        day_res = 7

    # count 是 08-10
    count = s.split('(')[1].split(')')[0].split('小节')[0]
    lesson_start = int(count.split('-')[0])
    lesson_count = int(count.split('-')[1]) - int(count.split('-')[0]) + 1
    return [day_res, lesson_start, lesson_count]


# 解析类似 星期一(14-14小节) 或 星期二(08-10小节) 或 星期一(14-14小节)星期二(08-10小节) 的字符串
# 返回三个数 [1, 8, 2] 表示周一，第8小节，2节课
def parse_time(s: str) -> list[list[int]]:
    res = []
    if s.count('星期') == 1:
        res.append(parse_single(s))

    # 星期一(14-14小节)星期二(08-10小节)
    elif s.count('星期') == 2:
        one = s.rsplit('星期', maxsplit=1)[0]
        two = s.split(')', maxsplit=1)[1]
        res.append(parse_single(one))
        res.append(parse_single(two))

    return res


# 解析类似 1-16(周) 或 1(周) 的字符串
def parse_week(s: str) -> list[int]:
    # num 是 1-16 或 1 这种形式
    num = s.split('(')[0]
    if '-' in num:
        start_week = int(num.split('-')[0])
        end_week = int(num.split('-')[1])
        return [start_week, end_week]
    else:
        return [int(num), int(num)]


def parse_final_table(my_course: list[list[dict]], courses: dict):
    for name, data in courses.items():
        start_week, end_week = data['week']
        # 星期下标偏移
        for week in range(start_week - 1, end_week):
            # 某一周可能有多节课
            for idx in range(len(data['time'])):
                my_course[week].append({
                    'name': name,
                    'code': data['code'],
                    'order': data['order'],
                    'teacher': data['teacher'],
                    'course_time': data['time'][idx],
                    'score': data['score'],
                    'room': data['room'][idx] if len(data['room']) != 0 else '',
                    'type': data['type'],
                    'stage': data['stage']
                })




def get_course_table() -> dict:
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'max-age=0',
        'Origin': 'http://202.119.81.112:9080',
        'Proxy-Connection': 'keep-alive',
        'Referer': 'http://202.119.81.112:9080/njlgdx/xskb/xskb_list.do?Ves632DSdyV=NEW_XSD_PYGL',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
    }

    params = {
        'Ves632DSdyV': 'NEW_XSD_PYGL',
    }

    data = [
        ('cj0701id', ''),
        ('zc', ''),
        ('demo', ''),
        ('xnxq01id', '2022-2023-2'),
        ('pageIndex', '1'),
    ]

    response = requests.post(
        'http://202.119.81.112:9080/njlgdx/xskb/xskb_list.do',
        params=params,
        cookies=cookies,
        headers=headers,
        data=data,
        verify=False,
    )

    text = response.text

    with open('class_table.html', 'w', encoding='utf-8') as fp:
        fp.write(text)

    tree = etree.HTML(text)

    blocks_top = tree.xpath('//div[@class="kbcontent"]')

    # 课程
    c_arr = []
    # 周次
    w_arr = []
    for block in blocks_top:
        for content in block.xpath('./text()'):
            if ('-----' not in content) and content != '\xa0' and content != '':
                c_arr.append(content)
        for content in block.xpath('./font[@title="周次(节次)"]/text()'):
            if content != ' ':
                w_arr.append(content)

    courses = dict()
    for i in range(len(c_arr)):
        courses[c_arr[i]] = {
            'week': w_arr[i]
        }

    # print(courses)

    blocks_bottom = tree.xpath('//*[@id="dataList"]//tr')
    for row in blocks_bottom:
        col = row.xpath('./td')
        if len(col) != 10:
            continue
        data = []
        for detail in col:
            content = detail.xpath('./text()')
            if len(content):
                data.append(content[0])
            else:
                data.append('')
        print(data[3])
        if courses.__contains__(data[3]):
            print('YES')
        else:
            print('NO')
        courses[data[3]]['code'] = data[1]
        courses[data[3]]['order'] = data[2]
        courses[data[3]]['teacher'] = data[4]
        courses[data[3]]['time'] = data[5]
        courses[data[3]]['score'] = data[6]
        courses[data[3]]['room'] = data[7]
        courses[data[3]]['type'] = data[8]
        courses[data[3]]['stage'] = data[9]

    # 打印初步结果
    # for name, data in courses.items():
    #     print(name)
    #     print(data)

    for _, data in courses.items():
        p_week = parse_week(data['week'])
        p_time = parse_time(data['time'])
        data['week'] = p_week
        data['time'] = p_time
        if len(p_time) == 2:
            data['room'] = data['room'].split(',')

    # 打印加工后的数据
    # for name, data in courses.items():
    #     print(name)
    #     print(data)

    my_courses = [[] for week in range(25)]
    parse_final_table(my_courses, courses)

    return my_courses


def main():
    start = time.time()
    login_jwc()
    my_course = get_course_table()

    week = 1
    for courses in my_course:
        print('第' + str(week) + '周')
        for course in courses:
            print(course, end=' ')
        print()
        week += 1

    end = time.time()
    print('运行时间：' + str(end - start) + '秒')


if __name__ == '__main__':
    main()

```



## 登录教务处最终版

```python
# -*-coding:utf-8 -*-

import time
from django.http import JsonResponse, HttpResponse
import ddddocr
from PIL import Image
from lxml import etree
from playwright.sync_api import Page, expect, sync_playwright


# 只解析 星期一(14-14小节 这样的字符串
def parse_single(s: str) -> list[int]:
    day = s.split('(')[0]
    if day == '星期一':
        day_res = 1
    elif day == '星期二':
        day_res = 2
    elif day == '星期三':
        day_res = 3
    elif day == '星期四':
        day_res = 4
    elif day == '星期五':
        day_res = 5
    elif day == '星期六':
        day_res = 6
    else:
        day_res = 7

    # count 是 08-10
    count = s.split('(')[1].split(')')[0].split('小节')[0]
    lesson_start = int(count.split('-')[0])
    lesson_count = int(count.split('-')[1]) - int(count.split('-')[0]) + 1
    return [day_res, lesson_start, lesson_count]


# 解析类似 星期一(14-14小节) 或 星期二(08-10小节) 或 星期一(14-14小节)星期二(08-10小节) 的字符串
# 返回三个数 [1, 8, 2] 表示周一，第8小节，2节课
def parse_time(s: str) -> list[list[int]]:
    res = []
    if s.count('星期') == 1:
        res.append(parse_single(s))

    # 星期一(14-14小节)星期二(08-10小节)
    elif s.count('星期') == 2:
        one = s.rsplit('星期', maxsplit=1)[0]
        two = s.split(')', maxsplit=1)[1]
        res.append(parse_single(one))
        res.append(parse_single(two))

    return res


# 解析类似 1-16(周) 或 1(周) 的字符串
def parse_week(s: str) -> list[int]:
    # num 是 1-16 或 1 这种形式
    num = s.split('(')[0]
    if '-' in num:
        start_week = int(num.split('-')[0])
        end_week = int(num.split('-')[1])
        return [start_week, end_week]
    else:
        return [int(num), int(num)]


def parse_final_table(my_course: list[list[dict]], courses: dict):
    for name, data in courses.items():
        start_week, end_week = data['week']
        # 星期下标偏移
        for week in range(start_week - 1, end_week):
            # 某一周可能有多节课
            for idx in range(len(data['time'])):
                my_course[week].append({
                    'name': name,
                    'code': data['code'],
                    'order': data['order'],
                    'teacher': data['teacher'],
                    'day': data['time'][idx][0],
                    'lesson_start': data['time'][idx][1],
                    'lesson_count': data['time'][idx][2],
                    'score': data['score'],
                    'room': data['room'][idx] if len(data['room']) != 0 else '',
                    'type': data['type'],
                    'stage': data['stage']
                })


def login_jwc(username: str, password: str):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context()
        page = context.new_page()
        page.goto("http://202.119.81.113:8080/")
        # 循环登录
        while True:
            page.locator("#userAccount").fill(username, timeout=10000)
            page.locator("#userPassword").fill(password, timeout=10000)
            # 验证码截图
            page.query_selector("#SafeCodeImg").screenshot(path="./code.png")

            # 获取ocr对象
            ocr = ddddocr.DdddOcr()

            # 获取图片
            img = Image.open("./code.png")

            # ocr对象识别图片
            safe_code = ocr.classification(img=img)
            print("验证码：" + safe_code)

            # 获取验证码输入框
            page.locator("#RANDOMCODE").fill(safe_code)

            # 点击 确认登录 按钮
            page.locator("#btnSubmit").click()

            if "验证码错误" in page.content():
                print("验证码错误！！")
            elif "该帐号不存在或密码错误" in page.content():
                print("该帐号不存在或密码错误!")
                return None
            elif "其它地方登录" in page.content():
                print("您的账号在其它地方登录")
            else:
                break

        cnt = 1
        while "我的桌面" not in page.content():
            # 阻塞cnt秒，依次递增
            time.sleep(cnt)
            # 刷新一次当前页面(感觉像是教务处的问题)
            page.reload()
            cnt += 1
            if cnt > 30:
                break

        # 访问教务处课表
        page.goto("http://202.119.81.112:9080/njlgdx/xskb/xskb_list.do?Ves632DSdyV=NEW_XSD_PYGL")

        with open('./kb.html', 'w', encoding='utf-8') as fp:
            fp.write(page.content())

        tree = etree.HTML(page.content())

        # 解析课表内容
        blocks_top = tree.xpath('//div[@class="kbcontent"]')

        # 课程
        c_arr = []
        # 周次
        w_arr = []
        for block in blocks_top:
            for content in block.xpath('./text()'):
                if ('-----' not in content) and content != '\xa0' and content != '':
                    c_arr.append(content)
            for content in block.xpath('./font[@title="周次(节次)"]/text()'):
                if content != ' ':
                    w_arr.append(content)

        courses = dict()
        for i in range(len(c_arr)):
            courses[c_arr[i]] = {
                'week': w_arr[i]
            }

        # print(courses)

        # blocks_bottom 是下面一整块表格
        blocks_bottom = tree.xpath('//*[@id="dataList"]//tr')
        # 遍历表格的每一行
        for row in blocks_bottom:
            # 获取每一行的每一列元素
            col = row.xpath('./td')
            if len(col) != 10:
                continue
            # data 是 一行课程数据
            data = []
            for detail in col:
                content = detail.xpath('./text()')
                if len(content) == 1:
                    data.append(content[0])
                elif len(content) == 2:
                    data.append(content)
                else:
                    data.append('')
            print(data[3])
            courses[data[3]]['code'] = data[1]
            courses[data[3]]['order'] = data[2]
            courses[data[3]]['teacher'] = data[4]
            courses[data[3]]['time'] = data[5]
            courses[data[3]]['score'] = data[6]
            courses[data[3]]['room'] = data[7].split(',')
            courses[data[3]]['type'] = data[8]
            courses[data[3]]['stage'] = data[9]

        # 打印初步结果
        print('打印初步结果')
        for name, data in courses.items():
            print(name)
            print(data)

        for _, data in courses.items():
            data['week'] = parse_week(data['week'])
            p_time = []

            # 解析 ['星期一(02-03小节)', '星期三(02-03小节)'] 或 星期四(04-05小节)
            if len(data['time']) == 2:
                for t in data['time']:
                    p_time.append(parse_single(t))
            else:
                p_time.append(parse_single(data['time']))

            data['time'] = p_time

        # 打印加工后的数据
        # print('打印加工后的数据')
        # for name, data in courses.items():
        #     print(name)
        #     print(data)

        # 格式如下：
        # 操作系统
        # {'week': [2, 9], 'code': '06022005', 'order': '1', 'teacher': '衷宜', 'time': [[1, 2, 2], [3, 2, 2]], 'score': '2', 'room': ['Ⅳ-A106', 'Ⅳ-A106'], 'type': '必修', 'stage': '预置'}
        # 或
        # {'week': [2, 13], 'code': '06029905', 'order': '0', 'teacher': '蔡志成', 'time': [[4, 4, 2]], 'score': '2', 'room': ['Ⅳ-B306'], 'type': '任选', 'stage': '一选'}

        my_courses = [[] for week in range(25)]
        parse_final_table(my_courses, courses)

        page.close()
        context.close()
        browser.close()

        return my_courses


def verify(request):
    if request.method != 'GET':
        return JsonResponse({
            'success': False,
            'msg': 'Only support GET method'
        })
    username = request.GET.get('username')
    password = request.GET.get('password')
    if username is None or password is None:
        return JsonResponse({
            'success': False,
            'msg': 'wrong params'
        })

    my_course = login_jwc(str(username), str(password))
    if my_course is None:
        return JsonResponse({
            'success': False,
            'msg': '用户名或密码错误'
        }, json_dumps_params={'ensure_ascii': False})

    return JsonResponse({
        'success': True,
        'data': my_course
    }, json_dumps_params={'ensure_ascii': False})

```





## 暂未整理

```python
import requests
import json
# 导入etree
from lxml import etree

import os
import ddddocr


def getFromBaidu():
    header = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Mobile Safari/537.36'
    }
    url = 'https://www.baidu.com/s?'
    wd = input('请输入关键字：')
    param = {
        "wd": wd
    }
    resp = requests.get(url=url, params=param, headers=header)
    pageText = resp.text
    fileName = wd + '.html'
    with open(fileName, 'w', encoding='utf-8') as fp:
        fp.write(pageText)
    print(fileName, "保存成功！")


def BaiduTranslation():
    header = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/99.0.4844.51 Mobile Safari/537.36 '
    }
    url = 'https://fanyi.baidu.com/sug'
    kw = input("请输入翻译内容：")
    param = {
        "kw": kw
    }
    resp = requests.post(url=url, params=param, headers=header)
    dicObj = resp.json()
    fileName = kw + '.txt'
    with open(fileName, 'w', encoding='utf-8') as fp:
        json.dump(dicObj, fp=fp)
    print(fileName, "翻译成功！")


def getFilm():
    header = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/99.0.4844.51 Mobile Safari/537.36 '
    }
    url = 'https://movie.douban.com/j/search_subjects?'
    param = {
        'type': 'movie',
        'tag': '最新',
        'source': 'index',
        'page_limit': 50,
        'page_start': 0
    }
    resp = requests.get(url=url, params=param, headers=header)
    dicObj = resp.json()
    with open('resp.txt', 'w', encoding='utf-8') as fp:
        json.dump(obj=dicObj, fp=fp)


def useTree():
    # from lxml import etree
    # 没有parser会导致乱码
    tree = etree.parse('resp.html', parser=etree.HTMLParser(encoding='utf-8'))

    # 从根节点开始定位
    title = tree.xpath('/html/head/title')
    # print(title)

    # // 表示多个层级
    div = tree.xpath('/html//div')
    # print(div)

    # 通过class定义标签
    person = tree.xpath('//div[@class="person"]')
    # print(person)

    # 通过索引定位，索引从1开始
    p = tree.xpath('//div[@class="person"]//li[2]/text()')
    # print(p)


def get58HouseInfo():
    header = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/99.0.4844.51 Mobile Safari/537.36 '
    }
    url = 'https://nj.58.com/ershoufang/'
    resp = requests.get(url=url, headers=header)
    pageText = resp.text
    # print(pageText)
    tree = etree.HTML(pageText)
    fp = open('./news.html', 'w', encoding='utf-8')
    fp.write(pageText)
    # print(tree)
    liList = tree.xpath('//div[@class="title-wrap lines2"]/span')
    print(liList)
    for li in liList:
        htmlTitle = li.xpath('./text()')[0]
        print(htmlTitle)


def getPic():
    # 设置请求头
    header = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/99.0.4844.51 Mobile Safari/537.36 '
    }
    # 设置目标访问地址
    url = 'https://www1.visualchina.com/creative-image/zhongguo/'
    # 发送get请求，并获取响应
    resp = requests.get(url=url, headers=header)
    # 获取响应中的内容（Unicode编码）
    pageText = resp.text
    # 创建文件，并写入数据
    with open('./news.html', 'w', encoding='utf-8') as fp:
        fp.write(pageText)

    # 将响应内容以HTML的方式加载到tree中
    tree = etree.HTML(pageText)

    # # 获取前后标签之间的值
    # liList = tree.xpath('//figure//span')
    # for li in liList:
    #     content = li.xpath('./text()')[0]
    #     print(content)

    if not os.path.exists('images'):
        os.mkdir(path='./images')
    # 获取标签内部的属性值
    imgList = tree.xpath('//figure//img')
    for img in imgList:
        imgName = img.xpath('./@alt')[0] + '.jpg'
        imgSrc = 'https:' + img.xpath('./@data-src')[0]
        print(imgSrc)
        imgData = requests.get(url=imgSrc, headers=header).content
        imgPath = './images/' + imgName
        with open(imgPath, 'wb') as fp:
            fp.write(imgData)
            print('成功下载', imgName)


def getFans():
    header = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/99.0.4844.51 Mobile Safari/537.36 '
    }
    url = 'https://space.bilibili.com/23947287/fans/fans'
    resp = requests.get(url=url, headers=header)
    pageText = resp.text
    print(pageText)

    with open('fanslist.html', 'w', encoding='utf-8') as fp:
        fp.write(pageText)
    tree = etree.HTML(pageText)
    li = tree.xpath('//span[@class="fans-name"]')
    for item in li:
        name = item.xpath('./text()')
        print(name)


def loginJWC():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/99.0.4844.51 Mobile Safari/537.36 ',
    }
    # 第一次请求地址
    url = 'http://202.119.81.113:8080'

    # 获取页面返回信息
    resp = requests.get(url=url, headers=headers)

    # 获取页面内容（默认使用Unicode编码方式展示内容）
    pageText = resp.text

    # 获取返回cookies（返回值为list<tuple>）
    cookies = resp.cookies.items()
    # 解析cookies，并加入请求头
    ck = cookies[0][0] + '=' + cookies[0][1]
    print(cookies)
    headers['Cookie'] = ck

    # 将html页面数据加载到tree中
    tree = etree.HTML(pageText)

    # 获取标签内部的属性对应的值（字符串形式）
    imgPath = url + tree.xpath('//img[@id="SafeCodeImg"]/@src')[0]

    # 以字节流的方式获取响应数据
    imgData = requests.get(url=imgPath, headers=headers).content
    with open('code.jpg', 'wb') as fp:
        fp.write(imgData)

    # 使用ocr破解验证码
    ocr = ddddocr.DdddOcr()
    randomCode = ocr.classification(imgData)
    print(randomCode)

    # 第二次请求地址
    url = 'http://202.119.81.113:8080/Logon.do?method=logon'
    # 设置post请求的携带参数
    data = {
        'USERNAME': '920106840520',
        'PASSWORD': '123456789qwe',
        'useDogCode': '',
        'RANDOMCODE': randomCode
    }
    resp = requests.post(url=url, data=data, headers=headers)
    pageText = resp.text

    with open('./resp.html', 'w', encoding='utf-8') as fp:
        fp.write(pageText)


def loginACW():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/99.0.4844.51 Mobile Safari/537.36 ',
    }
    # 第一次请求地址
    url = 'https://www.acwing.com/user/account/signin/'

    # 获取页面返回信息
    resp = requests.post(url=url, headers=headers)

    # 获取页面内容（默认使用Unicode编码方式展示内容）
    pageText = resp.text

    with open('./acw.html', 'w', encoding='utf-8') as fp:
        fp.write(pageText)


    # 将html页面数据加载到tree中
    tree = etree.HTML(pageText)
    value = tree.xpath('//input[@name="csrfmiddlewaretoken"]/@value')

    print(value)

    # 第二次请求地址
    url = 'http://202.119.81.113:8080/Logon.do?method=logon'
    # 设置post请求的携带参数
    data = {
        'USERNAME': '920106840520',
        'PASSWORD': '123456789qwe',
        'useDogCode': '',
        # 'RANDOMCODE': randomCode
    }
    resp = requests.post(url=url, data=data, headers=headers)
    pageText = resp.text


if __name__ == '__main__':
    # getFromBaidu()
    # BaiduTranslation()
    # getFilm()
    # useTree()
    # get58HouseInfo()
    # getPic()
    # getFans()
    # loginJWC()
    # testLogin()
    loginACW()
```

